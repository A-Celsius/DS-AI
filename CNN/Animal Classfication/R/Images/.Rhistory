a
world.cities
map(world.cities)
map(database = world2MapEnv)
map(database = "world2MapEnv")
map(database = world)
map(database = "world")
library(ggplot2)
dam <- data.frame(
category <- c("Major","Medium","Minor"),
number <- c(30,136,300))
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
library(maps)
india.cities
canada.cities
map.cities(x = canada.cities)
map.cities(x = world.cities, country = "Canada")
map(database = "world2MapEnv")
map(database = "world")
library(ggplot2)
dam <- data.frame(
category <- c("Major","Medium","Minor"),
number <- c(30,136,300))
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0)
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=2) +
coord_polar("y", start=0) +
theme_void()
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=2) +
coord_polar("y", start=0)
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=) +
coord_polar("y", start=0) +
theme_void()
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=) +
coord_polar("y", start=0)
+
theme_void()
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=) +
coord_polar("y", start=0) +
theme_void()
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0)
library(maps)
india.cities
canada.cities
map.cities(x = canada.cities)
map.cities(x = world.cities, country = "Canada")
map(database = "world2MapEnv")
map(database = "world")
#histogram
hist(number)
library(ggplot2)
dam <- data.frame(
category <- c("Major","Medium","Minor"),
number <- c(30,136,300))
#pie chart
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
#histogram
hist(number)
#histogram
hist(number,col = blue,border = black)
#histogram
hist(number,col = blue,border = black)
#histogram
hist(number,col = "blue",border = "black")
#histogram
hist(dam,col = "blue",border = "black")
#histogram
x <- c(1,2,3,4,1,2,1,3,4,5,2,4,5,2,1,5,2,4,2)
hist(x,col = "blue",border = "black")
source("~/R/Sem 4/1.R")
#Loading packages
library(party)
source("~/R/Sem 4/1.R")
install.packages("party")
source("~/R/Sem 4/1.R")
install.packages("party")
a
source("~/R/Sem 4/1.R")
# Printing something on screen
print("Hello World")
# Reading csv file
a <- read.csv("C:\\Users\\akshu\\Documents\\R\\Planets.csv")
a
# Printing something on screen
print("Hello World")
# Assignment
x <- "Hello"
# Printing something on screen
print("Hello World")
# Assignment
x <- "Hello"
# Reading csv file
a <- read.csv("C:\\Users\\akshu\\Documents\\R\\Planets.csv")
a
# Syntax for installing packages
install.packages("tidyr")
#Loading packages
library(tidyr)
source("~/R/Sem 4/2.R")
x <- 0
if (x > 0){
print("x is a positive number");
}else if (x < 0){
print("x is a negative number");
}else {
print("x is 0");
}
switch(2,'A','B','C','D','E')
switch(4,'A','B','C','D','E')
switch('E' ,'A' = 'Akash','B' = 'Bharat','C' = 'Chandan','D' = 'Dilip','E' = 'Ekanpreet')
source("~/R/Sem 4/2.R")
source("~/R/Sem 4/3.R")
source("~/R/Sem 4/4.R")
library(ggplot2)
dam <- data.frame(
category <- c("Major","Medium","Minor"),
number <- c(30,136,300))
#pie chart
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
#pie chart
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
#pie chart
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
#pie chart
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
#histogram
x <- c(1,2,3,4,1,2,1,3,4,5,2,4,5,2,1,5,2,4,2)
hist(x,col = "blue",border = "black")
library(ggplot2)
dam <- data.frame(
category <- c("Major","Medium","Minor"),
number <- c(30,136,300))
#pie chart
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
#histogram
x <- c(1,2,3,4,1,2,1,3,4,5,2,4,5,2,1,5,2,4,2)
source("~/R/Sem 4/4.R")
library(ggplot2)
dam <- data.frame(
category <- c("Major","Medium","Minor"),
number <- c(30,136,300))
#pie chart
ggplot(dam, aes(x="", y=number, fill=category)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0) +
theme_void()
#histogram
x <- c(1,2,3,4,1,2,1,3,4,5,2,4,5,2,1,5,2,4,2)
hist(x,col = "blue",border = "black")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
library(caTools)
dataset = read.csv("C:\\Users\\akshu\\Documents\\R\\IBM CLASS
\\Linear Regression\\linearsheet.csv")
# Splitting the dataset into the Training set and Test set
set.seed(124)
split = sample.split(dataset$Y, SplitRatio = .5)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = Y ~ X,
data = training_set)
summary(regressor)
y_pred =predict(regressor, newdata = test_set)
print(y_pred)
# Loading packages
library(ggplot2)
library(dplyr)
#plot graphs
ggplot() +
geom_point(aes(x = training_set$X, y = training_set$Y),
color = 'red') +
geom_line(aes(x= training_set$X, y = predict(regressor,
newdata = training_set)), color = 'blue') +
ggtitle('Y vs X (training set)') +
xlab('X') +
ylab('Y')
#Prediction values
y_pred
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/IBM CLASS/Linear Regression/Simple Linear Regression.R")
source("~/R/Logistic Regression/multiple-regression.R")
# Importing the dataset
dataset = read.csv("C:\\Users\\akshu\\Documents\\R\\data2.csv")
# Encoding categorical data
dataset$State = factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
dataset$State
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting Multiple Linear Regression to the Training set
regressor = lm(formula = Profit ~ .,
data = training_set)
regressor
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
y_pred
source("~/R/Logistic Regression/multiple-regression.R")
source("~/R/Logistic Regression/multiple-regression.R")
source("~/R/Logistic Regression/multiple-regression.R")
source("~/R/Logistic Regression/Logistic regression.R")
source("~/R/Logistic Regression/Logistic regression.R")
source("~/R/Logistic Regression/Logistic regression.R")
sprintf("Accuracy on test set is %s", accuracy_test)
source("~/R/Logistic Regression/Logistic regression.R")
source("~/R/Logistic Regression/Logistic regression.R")
source("~/R/Logistic Regression/Logistic regression.R")
source("~/R/Logistic Regression/Logistic regression.R")
source("~/R/Logistic Regression/Logistic regression.R")
source("~/R/IBM CLASS/Decision tree/Decisiontree.R")
source("~/R/IBM CLASS/Decision tree/Decisiontree.R")
source("~/R/Logistic Regression/knn.R")
source("~/R/Logistic Regression/knn.R")
source("~/R/Logistic Regression/knn.R")
source("~/R/Logistic Regression/k-means.R")
source("~/R/Logistic Regression/k-means.R")
library(maps)
canada.cities
map.cities(x = canada.cities)
map.cities(x = canada.cities)
map.cities(x = world.cities, country = "Canada")
map('nz')
map.cities()
map('nz')
map('italy')
map('italy.')
map('nz')
map('nzMapEnv')
map('nzMapEnv')
map('state')
install.packages(c("blob", "broom", "cli", "clipr", "colorspace", "crayon", "evaluate", "fansi", "glue", "haven", "jsonlite", "knitr", "magrittr", "matrixStats", "openssl", "plyr", "processx", "ps", "RColorBrewer", "Rcpp", "readxl", "rlang", "rmarkdown", "scales", "tidyselect", "tinytex", "tzdb", "uuid", "vctrs", "withr", "xfun", "yaml", "zoo"))
map('france')
map('canada.cities')
map('county')
library(maps)
map('france')
library(maps)
map('france')
library(maps)
map('france')
library(maps)
map('france')
library(maps)
map('france')
library(maps)
map('france')
map('nz')
library(maps)
map('world')
library(maps)
library(maps)
map('france')
install.packages("tensorflow")
tensorflow::install_tensorflow()
# Import our required libraries
library(rpart)
library(rpart.plot)
mushrooms <- read.csv("mushroom.csv", header = FALSE)
head(mushrooms)
head(mushrooms)
# Import our required libraries
library(rpart)
library(rpart.plot)
mushrooms <- read.csv("mushroom.csv", header = TRUE)
head(mushrooms)
head(mushrooms)
# Define the factor names for "Class"
levels(mushrooms$Class) <- c("Edible","Poisonous")
# Define the factor names for "Class"
levels(mushrooms$Class) <- c("Edible","Poisonous")
# Define the factor names for "odor"
levels(mushrooms$odor) <- c("Almonds","Anise","Creosote","Fishy","Foul","Musty","None","Pungent","Spicy")
# Define the factor names for "Class"
levels(mushrooms$class) <- c("Edible","Poisonous")
# Define the factor names for "odor"
levels(mushrooms$odor) <- c("Almonds","Anise","Creosote","Fishy","Foul","Musty","None","Pungent","Spicy")
# Define the factor names for "print"
levels(mushrooms$print) <- c("Black","Brown","Buff","Chocolate","Green","Orange","Purple","White","Yellow")
# Define the factor names for "print"
levels(mushrooms$spore.print.color) <- c("Black","Brown","Buff","Chocolate","Green","Orange","Purple","White","Yellow")
head(mushrooms)
# Create a classification decision tree using "Class" as the variable we want to predict and everything else as its predictors.
myDecisionTree <- rpart(Class ~ ., data = mushrooms, method = "class")
# Create a classification decision tree using "Class" as the variable we want to predict and everything else as its predictors.
myDecisionTree <- rpart(class ~ ., data = mushrooms, method = "class")
# Print out a summary of our created model.
print(myDecisionTree)
rpart.plot(myDecisionTree, type = 3, extra = 2, under = TRUE, faclen=5, cex = .75)
newCase  <- mushrooms[10,-1]
newCase
predict(myDecisionTree, newCase, type = "class")
train_ind <- sample(c(1:nrow(mushrooms)), size = 10)
## 75% of the sample size
n <- nrow(mushrooms)
smp_size <- floor(0.75 * n)
## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(c(1:n), size = smp_size)
mushrooms_train <- mushrooms[train_ind, ]
mushrooms_test <- mushrooms[-train_ind, ]
newDT <- rpart(Class ~ ., data = mushrooms_train, method = "class")
newDT <- rpart(class ~ ., data = mushrooms_train, method = "class")
head(result)
result <- predict(newDT, mushrooms_test[,-1], type = "class")
head(result)
head(mushrooms_test$Class)
head(mushrooms_test$class)
table(mushrooms_test$Class, result)
table(mushrooms_test$class, result)
rpart.plot(myDecisionTree, type = 1, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 1, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 2, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 3, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 4, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 0, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 1, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 0, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 1, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 2, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 3, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 4, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 6, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 4, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 10, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 1, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 1, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 2, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 3, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 4, under = TRUE, faclen=5, cex = .75)
rpart.plot(myDecisionTree, type = 5, extra = 2, under = TRUE, faclen=5, cex = .75)
newCase  <- mushrooms[10,-1]
newCase
source("~/R/k_d.R")
# Import our required libraries
library(rpart)
library(rpart.plot)
mushrooms <- read.csv("mushroom.csv", header = TRUE)
head(mushrooms)
# Define the factor names for "class"
levels(mushrooms$class) <- c("Edible","Poisonous")
# Define the factor names for "odor"
levels(mushrooms$odor) <- c("Almonds","Anise","Creosote","Fishy","Foul","Musty","None","Pungent","Spicy")
# Define the factor names for "spore.print.color"
levels(mushrooms$spore.print.color) <- c("Black","Brown","Buff","Chocolate","Green","Orange","Purple","White","Yellow")
head(mushrooms)
# Create a classification decision tree using "Class" as the variable we want to predict and everything else as its predictors.
myDecisionTree <- rpart(class ~ ., data = mushrooms, method = "class")
# Print out a summary of our created model.
print(myDecisionTree)
rpart.plot(myDecisionTree, type = 5, extra = 2, under = TRUE, faclen=5, cex = .75)
newCase  <- mushrooms[10,-1]
newCase
predict(myDecisionTree, newCase, type = "class")
train_ind <- sample(c(1:nrow(mushrooms)), size = 10)
## 75% of the sample size
n <- nrow(mushrooms)
smp_size <- floor(0.75 * n)
## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(c(1:n), size = smp_size)
mushrooms_train <- mushrooms[train_ind, ]
mushrooms_test <- mushrooms[-train_ind, ]
newDT <- rpart(class ~ ., data = mushrooms_train, method = "class")
result <- predict(newDT, mushrooms_test[,-1], type = "class")
head(result)
head(mushrooms_test$class)
table(mushrooms_test$class, result)
result
evaluate(mushrooms_test$class,result)
library(keras)
evaluate(mushrooms_test$class,result)
evaluate(mushrooms_test$class,result)
table(mushrooms_test$class, result)
summary(mushrooms)
summary(mushrooms)
source("~/R/1.R")
# READ IMAGES
setwd('C:\\Users\\akshu\\Documents\\R\\Project\\AVAS\\Images')
pics <- c('Dolphin (1).jpg' ,'Dolphin (2).jpg' ,'Dolphin (3).jpg' ,'Dolphin (4).jpg' ,'Dolphin (5).jpg' ,'Dolphin (6).jpg' ,'Dolphin (7).jpg' ,'Dolphin (8).jpg' ,'Dolphin (9).jpg' ,'Dolphin (10).jpg' ,'Dolphin (11).jpg' ,'Dolphin (12).jpg' ,'Dolphin (13).jpg' ,'Dolphin (14).jpg' ,'Dolphin (15).jpg' ,'Dolphin (16).jpg' ,'Dolphin (17).jpg' ,'Dolphin (18).jpg' ,'Dolphin (19).jpg' ,'Dolphin (20).jpg' ,'Dolphin (21).jpg' ,'Dolphin (22).jpg' ,'Dolphin (23).jpg' ,'Dolphin (24).jpg' ,'Dolphin (25).jpg' ,'Dolphin (26).jpg' ,'Dolphin (27).jpg' ,'Dolphin (28).jpg' ,'Dolphin (29).jpg' ,'Dolphin (30).jpg' ,'Dolphin (31).jpg' ,'Dolphin (32).jpg' ,'Dolphin (33).jpg' ,'Dolphin (34).jpg' ,'Dolphin (35).jpg' ,'Dolphin (36).jpg' ,'Dolphin (37).jpg' ,'Dolphin (38).jpg' ,'Dolphin (39).jpg' ,'Dolphin (40).jpg' ,'Dolphin (41).jpg' ,'Dolphin (42).jpg' ,'Dolphin (43).jpg' ,'Dolphin (44).jpg' ,'Dolphin (45).jpg' ,'Dolphin (46).jpg' ,'Dolphin (47).jpg' ,'Dolphin (48).jpg' ,'Dolphin (49).jpg' ,'Dolphin (50).jpg' ,'Dolphin (51).jpg' ,'Dolphin (52).jpg' ,'Dolphin (53).jpg' ,'Dolphin (54).jpg' ,'Dolphin (55).jpg' ,'Dolphin (56).jpg' ,'Dolphin (57).jpg' ,'Dolphin (58).jpg' ,'Dolphin (59).jpg' ,'Dolphin (60).jpg',
'Lion (1).jpg' ,'Lion (2).jpg' ,'Lion (3).jpg' ,'Lion (4).jpg' ,'Lion (5).jpg' ,'Lion (6).jpg' ,'Lion (7).jpg' ,'Lion (8).jpg' ,'Lion (9).jpg' ,'Lion (10).jpg' ,'Lion (11).jpg' ,'Lion (12).jpg' ,'Lion (13).jpg' ,'Lion (14).jpg' ,'Lion (15).jpg' ,'Lion (16).jpg' ,'Lion (17).jpg' ,'Lion (18).jpg' ,'Lion (19).jpg' ,'Lion (20).jpg' ,'Lion (21).jpg' ,'Lion (22).jpg' ,'Lion (23).jpg' ,'Lion (24).jpg' ,'Lion (25).jpg' ,'Lion (26).jpg' ,'Lion (27).jpg' ,'Lion (28).jpg' ,'Lion (29).jpg' ,'Lion (30).jpg' ,'Lion (31).jpg' ,'Lion (32).jpg' ,'Lion (33).jpg' ,'Lion (34).jpg' ,'Lion (35).jpg' ,'Lion (36).jpg' ,'Lion (37).jpg' ,'Lion (38).jpg' ,'Lion (39).jpg' ,'Lion (40).jpg' ,'Lion (41).jpg' ,'Lion (42).jpg' ,'Lion (43).jpg' ,'Lion (44).jpg' ,'Lion (45).jpg' ,'Lion (46).jpg' ,'Lion (47).jpg' ,'Lion (48).jpg' ,'Lion (49).jpg' ,'Lion (50).jpg' ,'Lion (51).jpg' ,'Lion (52).jpg' ,'Lion (53).jpg' ,'Lion (54).jpg' ,'Lion (55).jpg' ,'Lion (56).jpg' ,'Lion (57).jpg' ,'Lion (58).jpg' ,'Lion (59).jpg' ,'Lion (60).jpg')
myPics <- c()
for (i in 1:120) {
myPics[[i]] <- readImage(pics[i])
}
#Resize
for (i in 1:120) {
myPics[[i]] <- resize(myPics[[i]], 24, 24)
}
str(myPics)
#Reshape
for (i in 1:120) {
myPics[[i]] <- array_reshape(myPics[[i]], c(24, 24, 3))
}
str(myPics)
# Row Bind
trainx <- NULL;
for(i in 1:50){ trainx <- rbind(trainx,myPics[[i]])}
for(i in 61:110){ trainx <- rbind(trainx,myPics[[i]])}
str(trainx)
source("~/R/Project/AVAS/Project.R")
#Load Packages
library(EBImage)
library(keras)
# READ IMAGES
setwd('C:\\Users\\akshu\\Documents\\R\\Project\\AVAS\\Images')
pics <- c('Dolphin (1).jpg' ,'Dolphin (2).jpg' ,'Dolphin (3).jpg' ,'Dolphin (4).jpg' ,'Dolphin (5).jpg' ,'Dolphin (6).jpg' ,'Dolphin (7).jpg' ,'Dolphin (8).jpg' ,'Dolphin (9).jpg' ,'Dolphin (10).jpg' ,'Dolphin (11).jpg' ,'Dolphin (12).jpg' ,'Dolphin (13).jpg' ,'Dolphin (14).jpg' ,'Dolphin (15).jpg' ,'Dolphin (16).jpg' ,'Dolphin (17).jpg' ,'Dolphin (18).jpg' ,'Dolphin (19).jpg' ,'Dolphin (20).jpg' ,'Dolphin (21).jpg' ,'Dolphin (22).jpg' ,'Dolphin (23).jpg' ,'Dolphin (24).jpg' ,'Dolphin (25).jpg' ,'Dolphin (26).jpg' ,'Dolphin (27).jpg' ,'Dolphin (28).jpg' ,'Dolphin (29).jpg' ,'Dolphin (30).jpg' ,'Dolphin (31).jpg' ,'Dolphin (32).jpg' ,'Dolphin (33).jpg' ,'Dolphin (34).jpg' ,'Dolphin (35).jpg' ,'Dolphin (36).jpg' ,'Dolphin (37).jpg' ,'Dolphin (38).jpg' ,'Dolphin (39).jpg' ,'Dolphin (40).jpg' ,'Dolphin (41).jpg' ,'Dolphin (42).jpg' ,'Dolphin (43).jpg' ,'Dolphin (44).jpg' ,'Dolphin (45).jpg' ,'Dolphin (46).jpg' ,'Dolphin (47).jpg' ,'Dolphin (48).jpg' ,'Dolphin (49).jpg' ,'Dolphin (50).jpg' ,'Dolphin (51).jpg' ,'Dolphin (52).jpg' ,'Dolphin (53).jpg' ,'Dolphin (54).jpg' ,'Dolphin (55).jpg' ,'Dolphin (56).jpg' ,'Dolphin (57).jpg' ,'Dolphin (58).jpg' ,'Dolphin (59).jpg' ,'Dolphin (60).jpg',
'Lion (1).jpg' ,'Lion (2).jpg' ,'Lion (3).jpg' ,'Lion (4).jpg' ,'Lion (5).jpg' ,'Lion (6).jpg' ,'Lion (7).jpg' ,'Lion (8).jpg' ,'Lion (9).jpg' ,'Lion (10).jpg' ,'Lion (11).jpg' ,'Lion (12).jpg' ,'Lion (13).jpg' ,'Lion (14).jpg' ,'Lion (15).jpg' ,'Lion (16).jpg' ,'Lion (17).jpg' ,'Lion (18).jpg' ,'Lion (19).jpg' ,'Lion (20).jpg' ,'Lion (21).jpg' ,'Lion (22).jpg' ,'Lion (23).jpg' ,'Lion (24).jpg' ,'Lion (25).jpg' ,'Lion (26).jpg' ,'Lion (27).jpg' ,'Lion (28).jpg' ,'Lion (29).jpg' ,'Lion (30).jpg' ,'Lion (31).jpg' ,'Lion (32).jpg' ,'Lion (33).jpg' ,'Lion (34).jpg' ,'Lion (35).jpg' ,'Lion (36).jpg' ,'Lion (37).jpg' ,'Lion (38).jpg' ,'Lion (39).jpg' ,'Lion (40).jpg' ,'Lion (41).jpg' ,'Lion (42).jpg' ,'Lion (43).jpg' ,'Lion (44).jpg' ,'Lion (45).jpg' ,'Lion (46).jpg' ,'Lion (47).jpg' ,'Lion (48).jpg' ,'Lion (49).jpg' ,'Lion (50).jpg' ,'Lion (51).jpg' ,'Lion (52).jpg' ,'Lion (53).jpg' ,'Lion (54).jpg' ,'Lion (55).jpg' ,'Lion (56).jpg' ,'Lion (57).jpg' ,'Lion (58).jpg' ,'Lion (59).jpg' ,'Lion (60).jpg')
myPics <- c()
for (i in 1:120) {
myPics[[i]] <- readImage(pics[i])
}
#Resize
for (i in 1:120) {
myPics[[i]] <- resize(myPics[[i]], 24, 24)
}
str(myPics)
#Reshape
for (i in 1:120) {
myPics[[i]] <- array_reshape(myPics[[i]], c(24, 24, 3))
}
str(myPics)
# Row Bind
trainx <- NULL;
for(i in 1:50){ trainx <- rbind(trainx,myPics[[i]])}
for(i in 61:110){ trainx <- rbind(trainx,myPics[[i]])}
str(trainx)
testx <- NULL;
for (i in 51:60){ testx <- rbind(testx, myPics[[i]])}
for (i in 111:120){ testx <- rbind(testx, myPics[[i]])}
str(testx)
# 0 = Dolphin, 1 = Lion
trainy <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
testy <- c(0,0,0,0,0,0,0,0,0,0,
1,1,1,1,1,1,1,1,1,1)
# One hot encoding
trainlabels <- to_categorical(trainy)
testlabels <- to_categorical(testy)
#Model
model <- keras_model_sequential()
model %>%
layer_dense(units = 64,activation = 'relu',input_shape = c(1728)) %>%
layer_dense(units = 32,activation = 'relu') %>%
layer_dense(units = 16,activation = 'relu') %>%
layer_dense(units = 2, activation = 'softmax')
summary(model)
#Compile
model %>%
compile(loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy'))
#Fit Model
history <- model %>%
fit(trainx,
trainlabels,
epochs = 40,
batch_size = 30,
validation_split = 0.2)
plot(history)
# Evaluation & Prediction - train data
model %>% evaluate(trainx,trainlabels)
pred1 <- model %>% predict(trainx) %>% k_argmax()
source("~/R/Project/AVAS/Project.R")
